{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fd0aefc-42c6-48f3-b2da-023f5433cf72",
   "metadata": {},
   "source": [
    "### Test with advanced chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dc1322d9-56ef-472c-a17a-bf488bd0cbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import array\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# to generate id from text\n",
    "import hashlib\n",
    "\n",
    "import oci\n",
    "\n",
    "import tokenizers\n",
    "from tokenizers import Tokenizer\n",
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index.node_parser import SentenceSplitter\n",
    "\n",
    "import oracledb\n",
    "import ads\n",
    "\n",
    "# This is the wrapper for GenAI Embeddings\n",
    "from ads.llm import GenerativeAIEmbeddings\n",
    "\n",
    "from oci_utils import load_oci_config\n",
    "\n",
    "# this way we don't show & share\n",
    "from config_private import (\n",
    "    DB_USER,\n",
    "    DB_PWD,\n",
    "    DB_SERVICE,\n",
    "    DB_HOST_IP,\n",
    "    COMPARTMENT_OCID,\n",
    "    ENDPOINT,\n",
    ")\n",
    "\n",
    "#\n",
    "# Configs\n",
    "#\n",
    "from config import INPUT_FILES, EMBED_MODEL, EMBEDDINGS_BITS, ID_GEN_METHOD, TOKENIZER\n",
    "\n",
    "# to create embeddings in batch\n",
    "BATCH_SIZE = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "246d808e-a449-4db1-9dc4-497aca08e8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"covid19_treatment_guidelines.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e87590b7-9ec2-4ee1-9798-841d5c6da345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.replace(\"\\t\", \" \")\n",
    "    text = text.replace(\" -\\n\", \"\")\n",
    "    text = text.replace(\"-\\n\", \"\")\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "    # remove repeated blanks\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b333a385-18d0-482f-bf33-5217e45572f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edecf7f298b14dcda4d7db5a17d8e06e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pages = SimpleDirectoryReader(input_files=[INPUT_FILE]).load_data()\n",
    "\n",
    "for doc in pages:\n",
    "    doc.text = preprocess_text(doc.text)\n",
    "\n",
    "node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents(pages, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9adffc1-8c39-4971-bc5c-28cfc708aa26",
   "metadata": {},
   "source": [
    "#### count the number of token per page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "796588d3-7b4f-4ab6-97dc-7e522766ef6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohere_tokenizer = Tokenizer.from_pretrained(TOKENIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fbadba7f-7959-4a2e-8fbd-d80adef9639d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 993/993 [00:00<00:00, 1802.75it/s]\n"
     ]
    }
   ],
   "source": [
    "list_pages = []\n",
    "list_tokens = []\n",
    "\n",
    "i = 0\n",
    "for node in tqdm(nodes):\n",
    "    list_pages.append(i + 1)\n",
    "    list_tokens.append(len(cohere_tokenizer.encode(node.text)))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7e74e87b-2128-49b0-81e0-76b623b872c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_pages = {\"pages\": list_pages, \"tokens\": list_tokens}\n",
    "\n",
    "df_tokens = pd.DataFrame(dict_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "24f7dd8d-9269-4e0f-88df-f1fb3e10485a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pages</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>993.00000</td>\n",
       "      <td>993.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>497.00000</td>\n",
       "      <td>369.526687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>286.79871</td>\n",
       "      <td>120.007155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>249.00000</td>\n",
       "      <td>303.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>497.00000</td>\n",
       "      <td>423.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>745.00000</td>\n",
       "      <td>460.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>993.00000</td>\n",
       "      <td>502.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pages      tokens\n",
       "count  993.00000  993.000000\n",
       "mean   497.00000  369.526687\n",
       "std    286.79871  120.007155\n",
       "min      1.00000   26.000000\n",
       "25%    249.00000  303.000000\n",
       "50%    497.00000  423.000000\n",
       "75%    745.00000  460.000000\n",
       "max    993.00000  502.000000"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ec09aa-67d4-4e7d-9359-62b26b522841",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
