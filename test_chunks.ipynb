{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fd0aefc-42c6-48f3-b2da-023f5433cf72",
   "metadata": {},
   "source": [
    "### Test with advanced chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc1322d9-56ef-472c-a17a-bf488bd0cbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import array\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# to generate id from text\n",
    "import hashlib\n",
    "\n",
    "import oci\n",
    "\n",
    "import tokenizers\n",
    "from tokenizers import Tokenizer\n",
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index.node_parser import SentenceSplitter\n",
    "\n",
    "import oracledb\n",
    "import ads\n",
    "\n",
    "# This is the wrapper for GenAI Embeddings\n",
    "from ads.llm import GenerativeAIEmbeddings\n",
    "\n",
    "from oci_utils import load_oci_config\n",
    "\n",
    "# this way we don't show & share\n",
    "from config_private import (\n",
    "    DB_USER,\n",
    "    DB_PWD,\n",
    "    DB_SERVICE,\n",
    "    DB_HOST_IP,\n",
    "    COMPARTMENT_OCID,\n",
    "    ENDPOINT,\n",
    ")\n",
    "\n",
    "#\n",
    "# Configs\n",
    "#\n",
    "from config import (\n",
    "    INPUT_FILES,\n",
    "    EMBED_MODEL,\n",
    "    EMBEDDINGS_BITS,\n",
    "    ID_GEN_METHOD,\n",
    "    TOKENIZER,\n",
    "    MAX_CHUNK_SIZE,\n",
    ")\n",
    "\n",
    "# to create embeddings in batch\n",
    "BATCH_SIZE = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "246d808e-a449-4db1-9dc4-497aca08e8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"high-availability-23c.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e87590b7-9ec2-4ee1-9798-841d5c6da345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.replace(\"\\t\", \" \")\n",
    "    text = text.replace(\" -\\n\", \"\")\n",
    "    text = text.replace(\"-\\n\", \"\")\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "    # remove repeated blanks\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b333a385-18d0-482f-bf33-5217e45572f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 561/561 [00:00<00:00, 910.47it/s]\n"
     ]
    }
   ],
   "source": [
    "pages = SimpleDirectoryReader(input_files=[INPUT_FILE]).load_data()\n",
    "\n",
    "for doc in pages:\n",
    "    doc.text = preprocess_text(doc.text)\n",
    "\n",
    "node_parser = SentenceSplitter(chunk_size=MAX_CHUNK_SIZE, chunk_overlap=20)\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents(pages, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9adffc1-8c39-4971-bc5c-28cfc708aa26",
   "metadata": {},
   "source": [
    "#### count the number of token per page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "796588d3-7b4f-4ab6-97dc-7e522766ef6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohere_tokenizer = Tokenizer.from_pretrained(TOKENIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbadba7f-7959-4a2e-8fbd-d80adef9639d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1003/1003 [00:00<00:00, 2816.73it/s]\n"
     ]
    }
   ],
   "source": [
    "list_pages = []\n",
    "list_tokens = []\n",
    "\n",
    "i = 0\n",
    "for node in tqdm(nodes):\n",
    "    list_pages.append(i + 1)\n",
    "    list_tokens.append(len(cohere_tokenizer.encode(node.text)))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e74e87b-2128-49b0-81e0-76b623b872c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_pages = {\"pages\": list_pages, \"tokens\": list_tokens}\n",
    "\n",
    "df_tokens = pd.DataFrame(dict_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24f7dd8d-9269-4e0f-88df-f1fb3e10485a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pages</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1003.000000</td>\n",
       "      <td>1003.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>502.000000</td>\n",
       "      <td>263.057827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>289.685462</td>\n",
       "      <td>110.712187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>251.500000</td>\n",
       "      <td>170.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>502.000000</td>\n",
       "      <td>299.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>752.500000</td>\n",
       "      <td>358.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1003.000000</td>\n",
       "      <td>489.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             pages       tokens\n",
       "count  1003.000000  1003.000000\n",
       "mean    502.000000   263.057827\n",
       "std     289.685462   110.712187\n",
       "min       1.000000    15.000000\n",
       "25%     251.500000   170.000000\n",
       "50%     502.000000   299.000000\n",
       "75%     752.500000   358.000000\n",
       "max    1003.000000   489.000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ec09aa-67d4-4e7d-9359-62b26b522841",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
