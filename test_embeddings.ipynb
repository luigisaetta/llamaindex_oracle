{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbdcfa97-1cde-4244-9b5e-61b23e1af904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import re\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import array\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# to generate id from text\n",
    "import hashlib\n",
    "\n",
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index.node_parser import SentenceSplitter\n",
    "\n",
    "import oracledb\n",
    "import ads\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# This is the wrapper for GenAI Embeddings\n",
    "from ads.llm import GenerativeAIEmbeddings\n",
    "\n",
    "from oci_utils import load_oci_config\n",
    "\n",
    "# this way we don't show & share\n",
    "from config_private import (\n",
    "    DB_USER,\n",
    "    DB_PWD,\n",
    "    DB_SERVICE,\n",
    "    DB_HOST_IP,\n",
    "    COMPARTMENT_OCID,\n",
    "    ENDPOINT,\n",
    ")\n",
    "\n",
    "#\n",
    "# Configs\n",
    "#\n",
    "from config import (\n",
    "    INPUT_FILES,\n",
    "    EMBED_MODEL,\n",
    "    TOKENIZER,\n",
    "    EMBEDDINGS_BITS,\n",
    "    ID_GEN_METHOD,\n",
    "    ENABLE_CHUNKING,\n",
    "    MAX_CHUNK_SIZE,\n",
    "    CHUNK_OVERLAP,\n",
    ")\n",
    "\n",
    "# to create embeddings in batch\n",
    "BATCH_SIZE = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f682f48-c6b5-4646-be7f-f86a1ba65d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_id(nodes_list: List):\n",
    "    \"\"\"\n",
    "    get a list of nodes (pages, chunks) and generate the id\n",
    "\n",
    "    return: list of id\n",
    "    \"\"\"\n",
    "    if ID_GEN_METHOD == \"LLINDEX\":\n",
    "        nodes_ids = [doc.id_ for doc in nodes_list]\n",
    "    # this way generated hashing the page\n",
    "    if ID_GEN_METHOD == \"HASH\":\n",
    "        logging.info(\"Hashing to compute id...\")\n",
    "        nodes_ids = []\n",
    "        for doc in tqdm(nodes_list):\n",
    "            encoded_text = doc.text.encode()\n",
    "            hash_object = hashlib.sha256(encoded_text)\n",
    "            hash_hex = hash_object.hexdigest()\n",
    "            nodes_ids.append(hash_hex)\n",
    "\n",
    "    return nodes_ids\n",
    "\n",
    "\n",
    "# remove pages with num words < threshold\n",
    "def remove_short_pages(pages, threshold):\n",
    "    n_removed = 0\n",
    "    for pag in pages:\n",
    "        if len(pag.text.split(\" \")) < threshold:\n",
    "            pages.remove(pag)\n",
    "            n_removed += 1\n",
    "\n",
    "    logging.info(f\"Removed {n_removed} short pages...\")\n",
    "\n",
    "    return pages\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.replace(\"\\t\", \" \")\n",
    "    text = text.replace(\" -\\n\", \"\")\n",
    "    text = text.replace(\"-\\n\", \"\")\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "    # remove repeated blanks\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def read_and_split_in_pages(input_files):\n",
    "    pages = SimpleDirectoryReader(input_files=input_files).load_data()\n",
    "\n",
    "    logging.info(f\"Read total {len(pages)} pages...\")\n",
    "\n",
    "    # preprocess text\n",
    "    for doc in pages:\n",
    "        doc.text = preprocess_text(doc.text)\n",
    "\n",
    "    # remove pages with num words < threshold\n",
    "    pages = remove_short_pages(pages, threshold=10)\n",
    "\n",
    "    # create a list of text (these are the chuncks to be embedded and saved)\n",
    "    pages_text = [doc.text for doc in pages]\n",
    "\n",
    "    # 23/12 register the num of the page\n",
    "    # must be a string\n",
    "    pages_num = [doc.metadata[\"page_label\"] for doc in pages]\n",
    "\n",
    "    # extract list of id\n",
    "    # this way id have been generated by llama-index\n",
    "    # 08/01/2024 refactored\n",
    "    pages_id = generate_id(pages)\n",
    "\n",
    "    return pages_text, pages_id, pages_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d4d9fb8-bd4f-4edf-adb9-9d20dd403b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = [\"high-availability-23c.pdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55dc5beb-21f0-4d0c-8558-20e5859435a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-05 23:33:09,525 - INFO - Read total 561 pages...\n",
      "2024-02-05 23:33:09,565 - INFO - Removed 0 short pages...\n",
      "2024-02-05 23:33:09,566 - INFO - Hashing to compute id...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 561/561 [00:00<00:00, 244544.23it/s]\n"
     ]
    }
   ],
   "source": [
    "pages_text, pages_id, pages_num = read_and_split_in_pages(input_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "216e5647-b2da-42b9-b216-108b5f33b861",
   "metadata": {},
   "outputs": [],
   "source": [
    "oci_config = load_oci_config()\n",
    "\n",
    "# need to do this way\n",
    "api_keys_config = ads.auth.api_keys(oci_config)\n",
    "\n",
    "# the embedding client\n",
    "embed_model = GenerativeAIEmbeddings(\n",
    "    compartment_id=COMPARTMENT_OCID,\n",
    "    model=EMBED_MODEL,\n",
    "    auth=api_keys_config,\n",
    "    truncate=\"END\",\n",
    "    # Optionally you can specify keyword arguments for the OCI client, e.g. service_endpoint.\n",
    "    client_kwargs={\"service_endpoint\": ENDPOINT},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0e28dec-ae76-4a6f-9ce2-5ec13bfbc1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(embed_model, nodes_text):\n",
    "    cohere_tokenizer = Tokenizer.from_pretrained(TOKENIZER)\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(nodes_text), BATCH_SIZE)):\n",
    "        batch = nodes_text[i : i + BATCH_SIZE]\n",
    "\n",
    "        # here we compute embeddings for a batch\n",
    "        embeddings_batch = embed_model.embed_documents(batch)\n",
    "        # add to the final list\n",
    "        embeddings.extend(embeddings_batch)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03cfb48d-b20f-4770-9b29-7d5e881f0f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:18<00:00,  1.59it/s]\n"
     ]
    }
   ],
   "source": [
    "vet_embeddings = compute_embeddings(embed_model, pages_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3f6e25-af82-4daf-a209-5578bda1d109",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
