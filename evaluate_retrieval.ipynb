{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "840266dc-0ddc-47b8-99a7-74b04ef0acc5",
   "metadata": {},
   "source": [
    "### Evaluate Retrieval\n",
    "* modified using **OCI Data Science Model Deployment**\n",
    "* inspired by: https://docs.llamaindex.ai/en/stable/examples/evaluation/retrieval/retriever_eval.html\n",
    "* https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d824c1f5-6ead-4934-a75e-d2697fac0d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "\n",
    "# LLM\n",
    "from llama_index.llms import MistralAI\n",
    "\n",
    "\n",
    "# Retrievers\n",
    "from llama_index.retrievers import (\n",
    "    BaseRetriever,\n",
    "    VectorIndexRetriever,\n",
    ")\n",
    "\n",
    "from llama_index.indices.query.schema import QueryBundle, QueryType\n",
    "from llama_index.schema import NodeWithScore\n",
    "\n",
    "# Evaluator\n",
    "from llama_index.evaluation import (\n",
    "    generate_question_context_pairs,\n",
    "    EmbeddingQAFinetuneDataset,\n",
    ")\n",
    "from llama_index.evaluation import RetrieverEvaluator\n",
    "\n",
    "import ads\n",
    "\n",
    "from oci_utils import load_oci_config\n",
    "\n",
    "# rerankers\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "from oci_baai_reranker import OCIBAAIReranker\n",
    "from oci_llama_reranker import OCILLamaReranker\n",
    "# Embeddings\n",
    "from ads.llm import GenerativeAIEmbeddings\n",
    "\n",
    "from typing import List\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6fe379b-0479-4c4f-8197-00c34eca899b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config_private import COHERE_API_KEY, MISTRAL_API_KEY, COMPARTMENT_OCID, ENDPOINT\n",
    "from config import EMBED_MODEL, RERANKER_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd348876-090e-4c3e-861d-bf378b1453ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-01 18:01:48--  https://arxiv.org/pdf/2307.09288.pdf\n",
      "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.3.42, 151.101.67.42, ...\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13661300 (13M) [application/pdf]\n",
      "Saving to: ‘llama2.pdf’\n",
      "\n",
      "llama2.pdf          100%[===================>]  13,03M  2,25MB/s    in 5,8s    \n",
      "\n",
      "2024-01-01 18:01:54 (2,25 MB/s) - ‘llama2.pdf’ saved [13661300/13661300]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"llama2.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b52233a-6e3d-4c15-a431-e2baa2c74695",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(input_files=['llama2.pdf']).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4ce5015-1dbe-4de1-8b07-c18133b5ce5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with this doc should be max 35\n",
    "N_PAGES = 20\n",
    "\n",
    "node_parser = SimpleNodeParser.from_defaults(chunk_size=512)\n",
    "nodes = node_parser.get_nodes_from_documents(documents[2:N_PAGES+2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11ef98f1-1317-409f-aa3d-4e67e6e1547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt to generate questions\n",
    "qa_generate_prompt_tmpl = \"\"\"\\\n",
    "Context information is below.\n",
    "\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "\n",
    "Given the context information and not prior knowledge.\n",
    "generate only questions based on the below query.\n",
    "\n",
    "You are a Professor. Your task is to setup \\\n",
    "{num_questions_per_chunk} questions for an upcoming \\\n",
    "quiz/examination. The questions should be diverse in nature \\\n",
    "across the document. The questions should not contain options, not start with Q1/ Q2. \\\n",
    "Restrict the questions to the context information provided.\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2396bd12-c667-4140-9767-62b8f4c5d2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean the dataset\n",
    "def filter_qa_dataset(qa_dataset):\n",
    "    \"\"\"\n",
    "    Filters out queries from the qa_dataset that contain certain phrases and the corresponding\n",
    "    entries in the relevant_docs, and creates a new EmbeddingQAFinetuneDataset object with\n",
    "    the filtered data.\n",
    "\n",
    "    :param qa_dataset: An object that has 'queries', 'corpus', and 'relevant_docs' attributes.\n",
    "    :return: An EmbeddingQAFinetuneDataset object with the filtered queries, corpus and relevant_docs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract keys from queries and relevant_docs that need to be removed\n",
    "    queries_relevant_docs_keys_to_remove = {\n",
    "        k for k, v in qa_dataset.queries.items()\n",
    "        if 'Here are 2' in v or 'Here are two' in v\n",
    "    }\n",
    "\n",
    "    # Filter queries and relevant_docs using dictionary comprehensions\n",
    "    filtered_queries = {\n",
    "        k: v for k, v in qa_dataset.queries.items()\n",
    "        if k not in queries_relevant_docs_keys_to_remove\n",
    "    }\n",
    "    filtered_relevant_docs = {\n",
    "        k: v for k, v in qa_dataset.relevant_docs.items()\n",
    "        if k not in queries_relevant_docs_keys_to_remove\n",
    "    }\n",
    "\n",
    "    # Create a new instance of EmbeddingQAFinetuneDataset with the filtered data\n",
    "    return EmbeddingQAFinetuneDataset(\n",
    "        queries=filtered_queries,\n",
    "        corpus=qa_dataset.corpus,\n",
    "        relevant_docs=filtered_relevant_docs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18d16d0c-0ed6-457b-8015-30d0dc8551de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 58/58 [01:47<00:00,  1.85s/it]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# This LLM is used to generare qa dataset\n",
    "#\n",
    "llm = MistralAI(\n",
    "            api_key=MISTRAL_API_KEY,\n",
    "            model=\"mistral-small\",\n",
    "            temperature=0.2,\n",
    "            max_tokens=1024,\n",
    "        )\n",
    "\n",
    "qa_dataset = generate_question_context_pairs(\n",
    "    nodes, llm=llm, num_questions_per_chunk=2, qa_generate_prompt_tmpl=qa_generate_prompt_tmpl\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af359200-fbc7-4ed9-a875-cc38725a8273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out pairs with phrases `Here are 2 questions based on provided context`\n",
    "qa_dataset = filter_qa_dataset(qa_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7ba282e-ab98-482d-9013-e6102b7d7f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dataset.save_json(\"qa_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "959aa63e-bf53-4c0e-a544-0f13e90c7674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload dataset\n",
    "qa_dataset = EmbeddingQAFinetuneDataset.from_json(\"qa_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4a99cd51-4dc5-4298-905d-278ce5d19eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many queries\n",
    "queries = qa_dataset.queries.values()\n",
    "\n",
    "len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f6a1ad22-6e43-45cc-91ea-ca4d6d4ca739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain the role of Reinforcement Learning with Human Feedback (RLHF) in aligning Large Language Models with human preferences.\n"
     ]
    }
   ],
   "source": [
    "print(list(queries)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26722cb2-5f5d-4a4b-9d3b-75f4ec74d235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-01 18:19:43,190 - INFO - Created OCI reranker client...\n",
      "2024-01-01 18:19:43,190 - INFO - Region: eu-frankfurt-1...\n",
      "2024-01-01 18:19:43,190 - INFO - Deployment id: ocid1.datasciencemodeldeployment.oc1.eu-frankfurt-1.amaaaaaangencdyaokm6zawt3akgu3lr7u4hm4o4zrr64emfr3vi2qmzw2xa...\n"
     ]
    }
   ],
   "source": [
    "# to add OCI\n",
    "# this is the ID of the Model deployment\n",
    "ID = RERANKER_ID\n",
    "\n",
    "oci_config = load_oci_config()\n",
    "\n",
    "# need to do this way\n",
    "api_keys_config = ads.auth.api_keys(oci_config)\n",
    "\n",
    "baai_reranker = OCIBAAIReranker(\n",
    "    auth=api_keys_config, deployment_id=ID, region=\"eu-frankfurt-1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb777354-bba7-4ca7-9520-dec8b54e39ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_N = 4\n",
    "TOP_K = 6\n",
    "\n",
    "EMBEDDINGS = {\n",
    "    \"OCICohereV3\": GenerativeAIEmbeddings(\n",
    "        compartment_id=COMPARTMENT_OCID,\n",
    "        model=EMBED_MODEL,\n",
    "        auth=api_keys_config,\n",
    "        # Optionally you can specify keyword arguments for the OCI client\n",
    "        # e.g. service_endpoint.\n",
    "        client_kwargs={\"service_endpoint\": ENDPOINT},\n",
    "    )\n",
    "}\n",
    "\n",
    "RERANKERS = {\n",
    "    \"NOReranker\": \"None\",\n",
    "    \"CohereRerank\": CohereRerank(api_key=COHERE_API_KEY, top_n=TOP_N),\n",
    "    \"OCIBAAReranker\": OCILLamaReranker(oci_reranker=baai_reranker, top_n=TOP_N)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf1065cb-6019-4451-b889-1203fe7d6c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from: https://docs.llamaindex.ai/en/stable/examples/evaluation/retrieval/retriever_eval.html\n",
    "\n",
    "def display_results(embedding_name, reranker_name, eval_results):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "\n",
    "    metric_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        metric_dicts.append(metric_dict)\n",
    "\n",
    "    full_df = pd.DataFrame(metric_dicts)\n",
    "\n",
    "    hit_rate = full_df[\"hit_rate\"].mean()\n",
    "    mrr = full_df[\"mrr\"].mean()\n",
    "\n",
    "    metric_df = pd.DataFrame(\n",
    "        {\"Embedding\": [embedding_name], \"Reranker\": [reranker_name], \"hit_rate\": [hit_rate], \"mrr\": [mrr]}\n",
    "    )\n",
    "\n",
    "    return metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "366bfd60-c9b2-4ce2-8d9a-eb59b1d10d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever that adds a reranker\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: VectorIndexRetriever,\n",
    "        reranker = None\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self.reranker = reranker\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "\n",
    "        retrieved_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "\n",
    "        if reranker != 'None':\n",
    "            retrieved_nodes = self.reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n",
    "        else:\n",
    "            retrieved_nodes = retrieved_nodes[:TOP_N]\n",
    "\n",
    "        return retrieved_nodes\n",
    "\n",
    "    async def _aretrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Asynchronously retrieve nodes given query.\n",
    "\n",
    "        Implemented by the user.\n",
    "\n",
    "        \"\"\"\n",
    "        return self._retrieve(query_bundle)\n",
    "\n",
    "    async def aretrieve(self, str_or_query_bundle: QueryType) -> List[NodeWithScore]:\n",
    "        if isinstance(str_or_query_bundle, str):\n",
    "            str_or_query_bundle = QueryBundle(str_or_query_bundle)\n",
    "        return await self._aretrieve(str_or_query_bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f7afb66-a84c-4f01-ba22-8dc9094910df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n",
      "Running Evaluation for Embedding Model: OCICohereV3 and Reranker: NOReranker\n",
      "Running Evaluation for Embedding Model: OCICohereV3 and Reranker: CohereRerank\n",
      "Running Evaluation for Embedding Model: OCICohereV3 and Reranker: OCIBAAReranker\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame()\n",
    "\n",
    "# Loop over embeddings\n",
    "for embed_name, embed_model in EMBEDDINGS.items():\n",
    "\n",
    "    service_context = ServiceContext.from_defaults(llm=None, embed_model=embed_model)\n",
    "    \n",
    "    # base vector index to which we will add reranking\n",
    "    vector_index = VectorStoreIndex(nodes, service_context=service_context)\n",
    "    vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=TOP_K, \n",
    "                                            service_context=service_context)\n",
    "\n",
    "    # Loop over rerankers\n",
    "    for rerank_name, reranker in RERANKERS.items():\n",
    "        print(f\"Running Evaluation for Embedding Model: {embed_name} and Reranker: {rerank_name}\")\n",
    "\n",
    "        # Define Retriever\n",
    "        custom_retriever = CustomRetriever(vector_retriever, reranker)\n",
    "\n",
    "        metrics = [\"mrr\", \"hit_rate\"]\n",
    "        \n",
    "        retriever_evaluator = RetrieverEvaluator.from_metric_names(metrics, \n",
    "                                                                   retriever=custom_retriever\n",
    "        )\n",
    "        # here we do the evaluation on the dataset\n",
    "        eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)\n",
    "\n",
    "        current_df = display_results(embed_name, rerank_name, eval_results)\n",
    "        # concat to overall\n",
    "        results_df = pd.concat([results_df, current_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a25ce9ac-1e87-4b03-9ca5-daeb50e03d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Embedding</th>\n",
       "      <th>Reranker</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OCICohereV3</td>\n",
       "      <td>NOReranker</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.683480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OCICohereV3</td>\n",
       "      <td>CohereRerank</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.790205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OCICohereV3</td>\n",
       "      <td>OCIBAAReranker</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.775585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Embedding        Reranker  hit_rate       mrr\n",
       "0  OCICohereV3      NOReranker  0.842105  0.683480\n",
       "1  OCICohereV3    CohereRerank  0.894737  0.790205\n",
       "2  OCICohereV3  OCIBAAReranker  0.894737  0.775585"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7172dd05-5ea1-47f0-a526-c8565fc61bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"evaluate_reranker.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7084a4-fcc6-4ea3-ac1c-a73f57e1ac62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
